{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O4Vm-vIiLwk"
      },
      "source": [
        "# Walker2D\n",
        "\n",
        "You aim in this task is to train the agent to win in Walker2D game with Actor-Critic, Advantage Actor Critic (A2C), Trust-region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO). \n",
        "To solve the task feel free to transform the state and reward from the environment.\n",
        "\n",
        "**Scoring**: Calculating the average reward for 50 episodes. You goal is to gain more than 1000 points.\n",
        "\n",
        "**Submission format**: send you notebook and trained model in **zipped** folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read me, please:\n",
        "***I've had to make some changes in the code to compute the whole thing using 'GPU' device. This made training much faster and 30%+ better results within the same time as using 'CPU'***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0mS0Xd9hRIk",
        "outputId": "181a067d-1ce6-4050-ca5b-985b4e8c6ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import pybullet_envs\n",
        "from gym import make\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() \n",
        "                                                     else torch.FloatTensor)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***The cell down below is the test GPU allocation from [pybullet guide](https://colab.research.google.com/drive/1u6j7JOqM05vUUjpVp5VNk0pd8q-vqGlx )***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OS: win32\n",
            "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:30:19) [MSC v.1929 64 bit (AMD64)]\n",
            "GPUtil 1.4.0\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 18% | 14% |\n",
            "| ID | Name                               | Serial | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|  0 | NVIDIA GeForce RTX 3070 Laptop GPU | [N/A]  | GPU-b3f26e7e-2bdc-a2de-6d3b-d86b9359a503 ||       42C |       18% |          14% ||       8192MB |      1142MB |      6901MB || Enabled      | Enabled        |\n",
            "numGPUs= 1\n",
            "using CPU renderer (TinyRenderer)\n"
          ]
        }
      ],
      "source": [
        "#you can enable the GPU by changing the runtime\n",
        "import os\n",
        "os.environ['MESA_GL_VERSION_OVERRIDE'] = '3.3'\n",
        "os.environ['MESA_GLSL_VERSION_OVERRIDE'] = '330'\n",
        "import pybullet as p\n",
        "import pybullet_data as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pylab\n",
        "%matplotlib inline\n",
        "p.connect(p.DIRECT)\n",
        "#allow to find the assets (URDF, obj, textures etc)\n",
        "p.setAdditionalSearchPath(pd.getDataPath())\n",
        "#optionally enable GPU for faster rendering in pybullet.getCameraImage\n",
        "enableGPU = False\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "import GPUtil as GPU\n",
        "import sys\n",
        "# Get all device ids and their processing and memory utiliazion\n",
        "# (deviceIds, gpuUtil, memUtil) = GPU.getGPUs()\n",
        "\n",
        "# Print os and python version information\n",
        "print('OS: ' + sys.platform)\n",
        "print(sys.version)\n",
        "\n",
        "# Print package name and version number\n",
        "print(GPU.__name__ + ' ' + GPU.__version__)\n",
        "\n",
        "# Show the utilization of all GPUs in a nice table\n",
        "GPU.showUtilization()\n",
        "\n",
        "# Show all stats of all GPUs in a nice table\n",
        "GPU.showUtilization(all=True)\n",
        "\n",
        "# NOTE: If all your GPUs currently have a memory consumption larger than 1%,\n",
        "# this step will fail. It's not a bug! It is intended to do so, if it does not\n",
        "# find an available GPU.\n",
        "GPUs = GPU.getGPUs()\n",
        "numGPUs = len(GPU.getGPUs())\n",
        "print(\"numGPUs=\",numGPUs)\n",
        "if numGPUs > 0:\n",
        "  enableGPU = True\n",
        "eglPluginId = -1\n",
        "if enableGPU:\n",
        "  import pkgutil\n",
        "  egl = pkgutil.get_loader('eglRenderer')\n",
        "  if (egl):\n",
        "    eglPluginId = p.loadPlugin(egl.get_filename(), \"_eglRendererPlugin\")\n",
        "  else:\n",
        "    eglPluginId = p.loadPlugin(\"eglRendererPlugin\")\n",
        "\n",
        "if eglPluginId>=0:\n",
        "  print(\"Using GPU hardware (eglRenderer)\")  \n",
        "else:\n",
        "  print(\"using CPU renderer (TinyRenderer)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aUR0Fy4g5Bq"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9wHwdfYyg7JE"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"Walker2DBulletEnv-v0\"\n",
        "\n",
        "LAMBDA = 0.95\n",
        "GAMMA = 0.99\n",
        "\n",
        "ACTOR_LR = 2e-5\n",
        "CRITIC_LR = 1e-5\n",
        "\n",
        "CLIP = 0.2\n",
        "ENTROPY_COEF = 1e-2\n",
        "BATCHES_PER_UPDATE = 2048\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "MIN_TRANSITIONS_PER_UPDATE = 2048\n",
        "MIN_EPISODES_PER_UPDATE = 4\n",
        "\n",
        "ITERATIONS = 1000\n",
        "\n",
        "    \n",
        "def compute_lambda_returns_and_gae(trajectory):\n",
        "    lambda_returns = []\n",
        "    gae = []\n",
        "    last_lr = 0.\n",
        "    last_v = 0.\n",
        "    for _, _, r, _, v in reversed(trajectory):\n",
        "        ret = r + GAMMA * (last_v * (1 - LAMBDA) + last_lr * LAMBDA)\n",
        "        last_lr = ret\n",
        "        last_v = v\n",
        "        lambda_returns.append(last_lr)\n",
        "        gae.append(last_lr - v)\n",
        "    \n",
        "    # Each transition contains state, action, old action probability, value estimation and advantage estimation\n",
        "    return [(s, a, p, v, adv) for (s, a, _, p, _), v, adv in zip(trajectory, reversed(lambda_returns), reversed(gae))]\n",
        "    \n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        # Advice: use same log_sigma for all states to improve stability\n",
        "        # You can do this by defining log_sigma as nn.Parameter(torch.zeros(...))\n",
        "        self.model =  nn.Sequential(\n",
        "              nn.Linear(state_dim, 256),\n",
        "              nn.ELU(),\n",
        "              nn.Linear(256, 256),\n",
        "              nn.ELU(),\n",
        "              nn.Linear(256, action_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(action_dim))\n",
        "        \n",
        "    def compute_proba(self, state, action):\n",
        "        # Returns probability of action according to current policy and distribution of actions\n",
        "        ''' YOUR CODE HERE '''\n",
        "        mu = self.model(state)\n",
        "        dist = Normal(mu, torch.exp(self.sigma))\n",
        "        prob = torch.exp(dist.log_prob(action).sum(-1))\n",
        "        return prob, dist\n",
        "        \n",
        "    def act(self, state):\n",
        "        # Returns an action (with tanh), not-transformed action (without tanh) and distribution of non-transformed actions\n",
        "        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n",
        "        ''' YOUR CODE HERE '''\n",
        "        mu = self.model(state)\n",
        "        dist = Normal(mu, torch.exp(self.sigma))\n",
        "        action = dist.sample()\n",
        "        return torch.tanh(action), action, dist\n",
        "        \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        \n",
        "    def get_value(self, state):\n",
        "        return self.model(state)\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        self.actor_optim = Adam(self.actor.parameters(), ACTOR_LR)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), CRITIC_LR)\n",
        "\n",
        "    def update(self, trajectories):\n",
        "        transitions = [t for traj in trajectories for t in traj] # Turn a list of trajectories into list of transitions\n",
        "        state, action, old_prob, target_value, advantage = zip(*transitions)\n",
        "        state = np.array(state)\n",
        "        action = np.array(action)\n",
        "        old_prob = np.array(old_prob)\n",
        "        target_value = np.array(target_value)\n",
        "        advantage = np.array(advantage)\n",
        "        advnatage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
        "        \n",
        "        \n",
        "        for _ in range(BATCHES_PER_UPDATE):\n",
        "            idx = np.random.randint(0, len(transitions), BATCH_SIZE) # Choose random batch\n",
        "            s = torch.tensor(state[idx], device=device).float()\n",
        "            a = torch.tensor(action[idx], device=device).float()\n",
        "            op = torch.tensor(old_prob[idx], device=device).float() # Probability of the action in state s.t. old policy\n",
        "            v = torch.tensor(target_value[idx], device=device).float() # Estimated by lambda-returns \n",
        "            adv = torch.tensor(advantage[idx], device=device).float() # Estimated by generalized advantage estimation \n",
        "            \n",
        "            ''' YOUR CODE HERE '''\n",
        "            # TODO: Update actor here\n",
        "            # calculate ratios\n",
        "            new_prob, dist = self.actor.compute_proba(s, a)\n",
        "            ratio = new_prob / op\n",
        "\n",
        "            # actor_loss\n",
        "            surr_loss = ratio * adv\n",
        "            clipped_surr_loss = (\n",
        "                torch.clamp(ratio, 1.0 - CLIP, 1.0 + CLIP) * adv\n",
        "            )\n",
        "\n",
        "            # entropy\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            actor_loss = (\n",
        "                - torch.min(surr_loss, clipped_surr_loss).mean()\n",
        "                - entropy * ENTROPY_COEF\n",
        "            )\n",
        "\n",
        "            self.actor_optim.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            self.actor_optim.step()\n",
        "\n",
        "\n",
        "            # TODO: Update critic here\n",
        "            # critic_loss\n",
        "            value = self.critic.get_value(s).flatten()\n",
        "            # critic_loss = (v - value).pow(2).mean()\n",
        "            critic_loss = F.smooth_l1_loss(value, v)\n",
        "\n",
        "            # train critic\n",
        "            self.critic_optim.zero_grad()\n",
        "            critic_loss.backward(retain_graph=True)\n",
        "            self.critic_optim.step()\n",
        "            \n",
        "            \n",
        "    def get_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state]), device=device).float()\n",
        "            value = self.critic.get_value(state)\n",
        "        return value.cpu().item()\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state]), device=device).float()\n",
        "            action, pure_action, distr = self.actor.act(state)\n",
        "            prob = torch.exp(distr.log_prob(pure_action).sum(-1))\n",
        "        return action.cpu().numpy()[0], pure_action.cpu().numpy()[0], prob.cpu().item()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor, \"agent.pkl\")\n",
        "\n",
        "\n",
        "def evaluate_policy(env, agent, episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.\n",
        "        \n",
        "        while not done:\n",
        "            state, reward, done, _ = env.step(agent.act(state)[0])\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    return returns\n",
        "   \n",
        "\n",
        "def sample_episode(env, agent):\n",
        "    s = env.reset()\n",
        "    d = False\n",
        "    trajectory = []\n",
        "    while not d:\n",
        "        a, pa, p = agent.act(s)\n",
        "        v = agent.get_value(s)\n",
        "        ns, r, d, _ = env.step(a)\n",
        "        trajectory.append((s, pa, r, p, v))\n",
        "        s = ns\n",
        "    return compute_lambda_returns_and_gae(trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ELe0GF4uhKYD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\miniforge3\\envs\\torchenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: WARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\n",
            "  logger.warn(\n",
            "c:\\ProgramData\\miniforge3\\envs\\torchenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: WARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\n",
            "  logger.warn(\n",
            "c:\\ProgramData\\miniforge3\\envs\\torchenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: WARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\n",
            "  logger.warn(\n",
            "c:\\ProgramData\\miniforge3\\envs\\torchenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: WARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \n",
            "  logger.deprecation(\n",
            "\n",
            "Step: 10, Reward mean: 32.76445962179802, Reward std: 13.257043339272526, Episodes: 1280, Steps: 20634\n",
            "Step: 20, Reward mean: 38.772235054442135, Reward std: 13.820719706283594, Episodes: 1606, Steps: 41524\n",
            "Step: 30, Reward mean: 58.101272445375514, Reward std: 13.331322066129085, Episodes: 1872, Steps: 62481\n",
            "Step: 40, Reward mean: 54.69662415395578, Reward std: 6.798903638883825, Episodes: 2145, Steps: 83382\n",
            "Step: 50, Reward mean: 64.48028512960767, Reward std: 7.989975867493289, Episodes: 2418, Steps: 104143\n",
            "Step: 60, Reward mean: 66.52254532065476, Reward std: 5.044019096295827, Episodes: 2693, Steps: 124965\n",
            "Step: 70, Reward mean: 75.18439597885387, Reward std: 12.587622258590557, Episodes: 2977, Steps: 145822\n",
            "Step: 80, Reward mean: 71.91264332223625, Reward std: 15.715663688913914, Episodes: 3244, Steps: 166785\n",
            "Step: 90, Reward mean: 75.4565837763998, Reward std: 17.194107402261448, Episodes: 3480, Steps: 187709\n",
            "Step: 100, Reward mean: 200.43589067706142, Reward std: 165.97203787720028, Episodes: 3674, Steps: 209002\n",
            "Step: 110, Reward mean: 96.207210011452, Reward std: 24.530287358469902, Episodes: 3771, Steps: 231428\n",
            "Step: 120, Reward mean: 808.6561213671243, Reward std: 857.9247807760389, Episodes: 3832, Steps: 257831\n",
            "Step: 130, Reward mean: 342.86970442574466, Reward std: 217.4812043854221, Episodes: 3888, Steps: 320930\n",
            "Step: 140, Reward mean: 1042.1315935424184, Reward std: 698.2393864443476, Episodes: 3930, Steps: 368019\n",
            "\n",
            "Model was evaluated on step 150 with results:\n",
            "\tReward mean: 1042.1315935424184, Reward std: 698.2393864443476, Episodes: 3976, Steps: 411288\n"
          ]
        }
      ],
      "source": [
        "env = make(ENV_NAME)\n",
        "ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
        "state = env.reset()\n",
        "episodes_sampled = 0\n",
        "steps_sampled = 0\n",
        "\n",
        "for i in range(ITERATIONS):\n",
        "    trajectories = []\n",
        "    steps_ctn = 0\n",
        "    \n",
        "    while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n",
        "        traj = sample_episode(env, ppo)\n",
        "        steps_ctn += len(traj)\n",
        "        trajectories.append(traj)\n",
        "    episodes_sampled += len(trajectories)\n",
        "    steps_sampled += steps_ctn\n",
        "\n",
        "    ppo.update(trajectories)        \n",
        "    \n",
        "    if (i + 1) % (ITERATIONS//100) == 0:\n",
        "        rewards = evaluate_policy(env, ppo, 5)\n",
        "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n",
        "        ppo.save()\n",
        "        if np.mean(rewards) >= 1000: break\n",
        "\n",
        "print (f'\\nModel was evaluated on step {i+1} with results:\\n\\tReward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hne6AOIohb1W"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5pdqXy1lheAP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.Agent at 0x1ff6f667fa0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.model = torch.load(\"agent.pkl\")\n",
        "        \n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array(state), device=device).float()\n",
        "            ''' YOUR CODE HERE '''\n",
        "            actions = self.model(state)\n",
        "            return np.argmax(actions.numpy())\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "Agent()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('torchenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8bd06085491f71d2f1892e97fbe898d3c287d0435c5183672a053c96df2a845c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
